(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{89:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return a})),n.d(t,"metadata",(function(){return s})),n.d(t,"toc",(function(){return c})),n.d(t,"default",(function(){return u}));var o=n(3),r=n(7),i=(n(0),n(98)),a={id:"decision-tree",title:"Decision Tree",sidebar_label:"Decision Tree"},s={unversionedId:"EasyTrack/decision-tree",id:"EasyTrack/decision-tree",isDocsHomePage:!1,title:"Decision Tree",description:"When we talk about classification type problem statement, one of the most common algorithm that people tend to go with is Decision tree. Let's try to understand Decision tree and how does it work exactly. This algorithm can be used for both classification and regression but in most cases it is used for Classification. In decision tree we split our data into different nodes in the tree. For the splitting purpose we use something known as entropy that helps us select the right features to split our data inorder to make our decision tree. Entropy helps us to determine the purity of split. A pure subsplit means we either get a value of 0 or 1(Yes or No). The goal in Decision tree is to get to the leaf node as quickly as possible. For this purpose we need to select the right features and parameters in order to obtain the leaf node as quickly as possible. Whenever we perform a split, we need to calculate the purity of the split and this is where use we Entropy. Let's say we need to classify whether a car will be sold or not given it's different properties and features. Let's see how we can calculate the entropy and determine the split for the above case.",source:"@site/docs/EasyTrack/decisiontree.md",slug:"/EasyTrack/decision-tree",permalink:"/docs/EasyTrack/decision-tree",editUrl:"https://github.com/OneStep-elecTRON/docs/EasyTrack/decisiontree.md",version:"current",sidebar_label:"Decision Tree",sidebar:"learningTracksSidebar",previous:{title:"Introduction to Easy Track",permalink:"/docs/EasyTrack/"},next:{title:"K-Means",permalink:"/docs/EasyTrack/k-means"}},c=[],l={toc:c};function u(e){var t=e.components,n=Object(r.a)(e,["components"]);return Object(i.b)("wrapper",Object(o.a)({},l,n,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"When we talk about classification type problem statement, one of the most common algorithm that people tend to go with is Decision tree. Let's try to understand Decision tree and how does it work exactly. This algorithm can be used for both classification and regression but in most cases it is used for Classification. In decision tree we split our data into different nodes in the tree. For the splitting purpose we use something known as entropy that helps us select the right features to split our data inorder to make our decision tree. Entropy helps us to determine the purity of split. A pure subsplit means we either get a value of 0 or 1(Yes or No). The goal in Decision tree is to get to the leaf node as quickly as possible. For this purpose we need to select the right features and parameters in order to obtain the leaf node as quickly as possible. Whenever we perform a split, we need to calculate the purity of the split and this is where use we Entropy. Let's say we need to classify whether a car will be sold or not given it's different properties and features. Let's see how we can calculate the entropy and determine the split for the above case.",Object(i.b)("br",null)),Object(i.b)("p",null,Object(i.b)("em",{parentName:"p"},"[image]")),Object(i.b)("p",null," We also need to understand a term known as Information gain. We use information gain to device our root node for the tree. We take the average of all the entropies based on a specific split. On paper this is a long process but for our system it is a very small task. We need to select that split which has the highest information gain for making our decision tree. Once we have our root node, we can either calculate the entropy for rest of the splits and use those splits which have the least entropy or go with a faster approach and use Gini Impurity. Gini impurity is used over entropy in ensemble as it is faster and takes less time as it doesn't contain any logarithmic calculation in it's formula which usually takes more time to compute."),Object(i.b)("p",null,Object(i.b)("em",{parentName:"p"},"[image]")),Object(i.b)("p",null,"Decision trees have one disadvantage. They suffer from the problem of overfitting as the decision trees tend to do perform very well on training data but fails to perform on testing data. This is what we call having low bias and high variance. This problem can be overcome with help of decision tree pruning or just using Random forests. We will learn about them next as they are based on Decision trees. Now we know how decision trees work so let's quickly summarize everything we just learnt."),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Decision trees can be used for both classification and regression problem statement but it is mostly used for classification."),Object(i.b)("li",{parentName:"ul"},"We make use of Information gain to get the Root node and then use either Entropy or Gini impurity to determine the further splits and to determine what features to use for the split. We choose the value which is lower when we compare entropies of 2 nodes and these values will be between 0 and 1."),Object(i.b)("li",{parentName:"ul"},"We keep splitting untill we reach the leaf node, the goal is to reach this leaf node as quickly as possible."),Object(i.b)("li",{parentName:"ul"},"Decision trees have low bias and high variance which is the condition for overfitting. We can eliminate this by using Decision tree pruning or using Random forest about which we will study in the next section.")),Object(i.b)("p",null,"With this we come to an end to the Decision Trees theory and we can go on to the Notebooks and Hands-On exercises."))}u.isMDXComponent=!0},98:function(e,t,n){"use strict";n.d(t,"a",(function(){return p})),n.d(t,"b",(function(){return f}));var o=n(0),r=n.n(o);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=r.a.createContext({}),u=function(e){var t=r.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=u(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},d=r.a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,a=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),p=u(n),d=o,f=p["".concat(a,".").concat(d)]||p[d]||h[d]||i;return n?r.a.createElement(f,s(s({ref:t},l),{},{components:n})):r.a.createElement(f,s({ref:t},l))}));function f(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=d;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:o,a[1]=s;for(var l=2;l<i;l++)a[l]=n[l];return r.a.createElement.apply(null,a)}return r.a.createElement.apply(null,n)}d.displayName="MDXCreateElement"}}]);